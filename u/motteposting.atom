<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom"><title type="text">u/motteposting - r/TheMotte</title><id>https://themotte.srid.ca/</id><updated>2022-08-06T06:17:23Z</updated><link href="https://themotte.srid.ca/u/motteposting" rel="self"/><entry><id>http://old.reddit.com/r/TheMotte/comments/wda188/culture_war_roundup_for_the_week_of_august_01_2022/ij5mky5/?sort=confidence</id><title type="text">[CW] **AI and the Irreducibility of Practical Reason**

I really enjoy pure math. I f</title><updated>2022-08-06T06:17:23Z</updated><author><name>motteposting</name></author><category label="CW" scheme="https://themotte.srid.ca/cw" term="Culture War"/><content type="text">**AI and the Irreducibility of Practical Reason**

I really enjoy pure math. I find it beautiful and fascinating. If I were so lucky as to have a world-class talent for number theory or something (alas, far from it), then I&#39;d probably be doing that instead. But with the advent of formal proof-assistants like Coq and given the unique amenability of math research to machine-tractable formats, I don&#39;t think that pure math will be any sort of holdout against a scale revolution if one occurs. (Moravec&#39;s paradox also applies here.)

This kind of makes me sad. On the one hand, the enormous progress on questions like the Riemann hypothesis or the Langlands Program that would be made would be amazing to see. On the other, most people think of pure math as the acme of human intellectual activity, but AI mathematicians seem to promise the reduction of all humanity to the status of students reading proofs out of a textbook. I guess that then we can put people like Terry Tao and Peter Scholze to work on more economically-productive uses of their talents. And someone will have to check (some of) the proofs that these AIs spit out, or at least the integrity of their proof-assistant software. But that&#39;s tinkering at the margins, not acting as the main player. 

Reflecting on this led me to wonder what the most &#34;future-proof&#34; area of human endeavor is. It isn&#39;t enough for a given subject to require lots of brainpower to excel in, or pure math would be safe. I think that practical deliberation is just about the only thing that is essentially impossible to automate. Although I could spin up a deep-learning program to spit out any number of arguments for or against any given course of action, I am always the only one who can give any definitive answer to the question, &#34;What ought *I* do?&#34; The only possible answers consist of *my* choices. Indeed, the very decision to look to some AI for advice on the matter would itself require some prior deliberation and choice on my part *first*. Human *agency* is essentially irreducible and inalienable. This is the truth of Sartre&#39;s assertion, &#34;Man is *condemned* to be free.&#34;

But this presents something of a problem for those who think that friendly AGI would solve all of our problems. Suppose that AGI doesn&#39;t kill us all, Yud et al.&#39;s dooming notwithstanding, and instead, everything goes as well as could be expected, at least materially speaking. No one wants for anything and the answer to most any empirical question is just a query away. All the same, the question remains, &#34;Well, shit, what do we do now?&#34; I think that this might be something of a &#34;dog that caught the car&#34; moment for a lot of people if it did come. It&#39;s a bit like winning the lottery after being poor: you never need want again, but now that you no longer have to worry about simply surviving, the deeper puzzle rears its head: &#34;What exactly am I living *for*?&#34; Is the end of striving and struggle, the automation of everything but forming our own preferences, really what we should want?

In *The Myth of Sisyphus*, Camus says, &#34;There is only one serious philosophical problem and that is suicide, judging whether or not life is or is not worth living is to answer the fundamental question in Philosophy.&#34; I wonder how many people in an AI-created Eden, if they truly committed to trying to answer that question, at long last deprived of all exigencies in life that might force them to think about anything else, would conclude that life there was worth living? Would effortless, endless prolongation of this finite life and its finite pursuits be enough for perpetuity? To me, it sounds rather like Hell. In any case, there&#39;s no AI that can evaluate that question *for* me.

This perspective brings out what I think is an interesting insight into people&#39;s differing orientations towards questions about AI. If you think that the primary problem of life is material or logistical, of simply having the means of satisfying preferences or generating satisfaction, then you are probably the most enthusiastic about friendly AI and the most afraid of AI disasters. For AGI is essentially the tool to end all tools, whatever else it may be. So if the best thing in life is (having the tools) to get what you want and the worst is being deprived of it, then AI going well means that things go as well as they could, and AI going badly means that they go as badly as they could. This probably goes a long way to explain why the ratsphere tends to attract utilitarians and vice-versa.

By contrast, if &#34;what we ought to do&#34; is something that is deeply and necessarily interwoven with human agency, as philosophers like Kant argue, and as has been the typical Christian line (along with most other world religions), then the ethical implications of AI are probably far less dramatic. After all, what good is it to increase preference satisfaction or economic output 1000x if what actually matters is having a good will or loving your neighbor with your whole heart and you suck at those things? (I know that I do...)

This raises a broader question: What is the *ultimate* point of thinking about what to do, how to act, etc.? Is reason a &#34;slave of the passions,&#34; as Hume claimed: simply a means to satisfy primal, a-rational drives with which we are saddled by brute psychology or biology? This would seem to comport with LW talk of &#34;terminal values,&#34; but that usually seems like more of a defense mechanism against having to give actual arguments for one&#39;s moral judgments than any sophisticated metaethics. Or does reason have practical principles all its own, adherence to which is rationally required (largely) independently of its compatibility with satisfying our pre-moral wants? Again, I think that the line that one takes will tend to influence how one thinks about technology and the moral valence of technological progress, with the former perspective tending to see it as a moral imperative and the latter as morally neutral, all else equal. What do you think?</content><link href="http://old.reddit.com/r/TheMotte/comments/wda188/culture_war_roundup_for_the_week_of_august_01_2022/ij5mky5/?sort=confidence"/></entry></feed>
<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom"><title type="text">u/gardenofjew - r/TheMotte</title><id>https://themotte.srid.ca/</id><updated>2022-08-31T00:57:07Z</updated><link href="https://themotte.srid.ca/u/gardenofjew" rel="self"/><entry><id>http://old.reddit.com/r/TheMotte/comments/x0hc17/culture_war_roundup_for_the_week_of_august_29_2022/imgofrm/?sort=confidence</id><title type="text">[CW] **Is &#34;Paperclipping&#34; a fringe idea?**

Lately, u/ilforte  has been banging the d</title><updated>2022-08-31T00:57:07Z</updated><author><name>gardenofjew</name></author><category label="CW" scheme="https://themotte.srid.ca/cw" term="Culture War"/><content type="text">**Is &#34;Paperclipping&#34; a fringe idea?**

Lately, u/ilforte  has been banging the drum of [paraphrasing] &#34;worrying about alignment too much at the expense of a high-freedom future is terrible, Yud is deeply paranoid which colors everything else he writes about, etc.&#34; &#34;Here is some suggestive evidence that Ilforte is mistaken on AGI Doom being a fringe/LW-only/Yud!/etc. idea:

1. DeepMind&#39;s alignment team [engaging](https://www.lesswrong.com/posts/qJgz2YapqpFEDTLKn/deepmind-alignment-team-opinions-on-agi-ruin-arguments) with Yud&#39;s List of Lethalities post. I don&#39;t detect much sneering, mostly specific disagreements with specific things.  

2. Sam Altman (OpenAI) re-tweeting that list of lethalities post (would link but short on time rn, easily found if you scroll back far enough)

3. Stability AI (of Stable Diffusion fame) is related to EleutherAI in some capacity. There&#39;s definitely a lot of personnel overlap. Emad (head of Stability AI, someone Ilforte has spoken very fondly of) has retweeted various LW posts ([1)](https://twitter.com/EMostaque/status/1563715319566409729) and has [retweeted this job ad](https://twitter.com/EMostaque/status/1512405527350943746) from Conjecture, the Alignment focused spin-off of EleutherAI, founded by Connor Leahy, seems to have alignment takes that are not far from Yud&#39;s, as you can tell if you either watch his interview on a [recent podcast](https://www.youtube.com/watch?v=Oz4G9zrlAGs&amp;amp;ab_channel=TheInsideView) or simply read his feed. 

note: These are three differnet organizations, all with relatively diverse views on AI, especially Stability AI vs the others. 


**What is the point of this post?**

Ilforte is an extremely eloquent writer. He is probably the most talented (certainly the most compelling! though IMO his prose is often far from lucid...) prose writer currently active on /themotte. And yet he seems to have a completely evidence-free view on the possible danger of AGI, has a deeply suspicious view of Yud, who, for all of his many flaws, is so bad at hiding his own views that he spends all of his weirdness points debating PR-toxic topics like baby qualia-- (you can accuse Yud of a few things, but a talented dissassembler for the sake of other goals he definitely is not...)

Ilforte seems to have decided to direct all of his immense verbal talent to deriding pessimistic takes on AGI, and I think this bad. I think a redteaming of paranoid takes on AI is great, but Ilforte has often made his arguments mostly through suspicion of AI-suspicious people.

I think its time someone pointed at this argument directly and engaged with it. I&#39;m not that person, unfortunately, because I&#39;m drowning in work rn, but I&#39;m planting a flag here and hoping Ilforte and others can debate productively about ways that [suspicious of AGI] and [suspicious of those who are suspicious of AGI] can productively disagree and understand the world better.</content><link href="http://old.reddit.com/r/TheMotte/comments/x0hc17/culture_war_roundup_for_the_week_of_august_29_2022/imgofrm/?sort=confidence"/></entry></feed>